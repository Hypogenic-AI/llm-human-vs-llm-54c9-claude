\begin{abstract}
Large Language Models (LLMs) are increasingly deployed in systems where they receive prompts from both humans and other LLMs. We investigate whether LLMs exhibit different response behaviors when presented with prompts written in a human style versus prompts written in a characteristic LLM style, while controlling for semantic content. We construct a dataset of 50 diverse questions, each paired with stylistically manipulated prompts---one informal and colloquial (human-style) and one formal and structured (LLM-style)---and query GPT-4.1-mini and Claude Sonnet 4 with both versions. Our analysis reveals that LLMs significantly alter their response behavior based on prompt style: responses to LLM-style prompts are 66\% longer on average (Cohen's $d = 2.07$, $p < 0.0001$), use 120\% more bullet points ($d = 1.44$), exhibit higher reading difficulty (Flesch-Kincaid Grade increases from 13.4 to 19.6), and show reduced vocabulary diversity. These effects are consistent across both model families tested. We term this phenomenon \emph{style mirroring}---the tendency of LLMs to adapt their output style to match the stylistic characteristics of the input prompt, analogous to conversational accommodation in human communication. Our findings have implications for prompt engineering, multi-agent system design, and AI safety evaluation, suggesting that stylistic framing is an important and underexplored dimension of LLM behavior.
\end{abstract}
