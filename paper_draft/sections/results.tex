\section{Results}
\label{sec:results}

Our experiments reveal strong evidence that LLMs alter their response behavior based on prompt style. We present aggregate results across both models, followed by model-specific analysis.

\subsection{Main Results}

Table~\ref{tab:main_results} presents the primary findings across all 100 response pairs (50 questions $\times$ 2 models).

\begin{table}[t]
\centering
\caption{Summary statistics comparing responses to \human{} versus \llmstyle{} prompts. Results are aggregated across both GPT-4.1-mini and Claude Sonnet 4. Effect sizes (Cohen's $d$) and $p$-values are from paired $t$-tests with Bonferroni correction ($\alpha_{\text{adj}} = 0.005$).}
\label{tab:main_results}
\small
\begin{tabular}{lccccc}
\hline
\textbf{Feature} & \textbf{\human{}} & \textbf{\llmstyle{}} & \textbf{Diff.} & \textbf{Cohen's $d$} & \textbf{$p$-value} \\
\hline
Word Count & 194.6 $\pm$ 55.9 & 323.1 $\pm$ 36.9 & +128.5 & \textbf{2.07} & $<$0.0001*** \\
Sentence Count & 12.4 $\pm$ 7.7 & 19.4 $\pm$ 10.9 & +7.0 & \textbf{0.75} & $<$0.0001*** \\
Avg Word Length & 5.44 $\pm$ 0.46 & 6.06 $\pm$ 0.52 & +0.62 & \textbf{1.06} & $<$0.0001*** \\
Avg Sentence Length & 21.3 $\pm$ 14.6 & 30.0 $\pm$ 37.1 & +8.7 & 0.25 & 0.14 \\
Type-Token Ratio & 0.72 $\pm$ 0.06 & 0.64 $\pm$ 0.05 & $-$0.07 & \textbf{$-$1.06} & $<$0.0001*** \\
Flesch Reading Ease & 38.8 $\pm$ 21.6 & 6.4 $\pm$ 41.4 & $-$32.4 & \textbf{$-$0.82} & $<$0.0001*** \\
Flesch-Kincaid Grade & 13.4 $\pm$ 5.6 & 19.6 $\pm$ 13.5 & +6.2 & \textbf{0.48} & $<$0.0001*** \\
Formal Word Ratio & 0.08\% $\pm$ 0.20\% & 0.22\% $\pm$ 0.20\% & +0.14\% & \textbf{0.50} & $<$0.0001*** \\
Bullet Points & 8.5 $\pm$ 5.7 & 18.6 $\pm$ 7.0 & +10.2 & \textbf{1.44} & $<$0.0001*** \\
Logical Connectors & 0.11 $\pm$ 0.31 & 0.03 $\pm$ 0.17 & $-$0.08 & $-$0.22 & 0.32 \\
\hline
\end{tabular}
\vspace{1mm}
\raggedright\footnotesize{*** indicates $p < 0.0001$ after Bonferroni correction. Bold Cohen's $d$ values indicate medium or large effect sizes ($|d| \geq 0.5$).}
\end{table}

\paragraph{Key Finding 1: Response Length.} Responses to \llmstyle{} prompts are dramatically longer, averaging 323.1 words compared to 194.6 words for \human{} prompts---a 66\% increase. This effect is very large (Cohen's $d = 2.07$) and highly significant ($p < 0.0001$). Figure~\ref{fig:length_comparison} visualizes this difference.

\paragraph{Key Finding 2: Structural Organization.} \llmstyle{} prompts elicit responses with substantially more bullet points (18.6 vs. 8.5, a 120\% increase, $d = 1.44$). This indicates that LLMs mirror the structural expectations implied by formal prompts by organizing their responses with headers, lists, and sections.

\paragraph{Key Finding 3: Reading Difficulty.} Responses to \llmstyle{} prompts are considerably harder to read:
\begin{itemize}
    \item Flesch Reading Ease drops from 38.8 (``difficult'') to 6.4 (``very confusing'')
    \item Flesch-Kincaid Grade rises from 13.4 (college level) to 19.6 (graduate level)
\end{itemize}
This suggests that formal prompts trigger more complex, academic-style responses.

\paragraph{Key Finding 4: Vocabulary Patterns.} Average word length increases significantly (5.44 to 6.06 characters, $d = 1.06$), while type-token ratio decreases (0.72 to 0.64, $d = -1.06$). This pattern---longer words but less vocabulary diversity---mirrors the characteristics of LLM-generated text identified in detection literature \citep{guo2023hc3}.

\begin{figure}[t]
\centering
\includegraphics[width=0.85\textwidth]{figures/response_length_comparison.png}
\caption{Distribution of response word counts for \human{} versus \llmstyle{} prompts. Responses to LLM-style prompts are consistently longer across both models, with minimal overlap between distributions.}
\label{fig:length_comparison}
\end{figure}

\subsection{Effect Size Analysis}

Figure~\ref{fig:effect_sizes} presents Cohen's $d$ effect sizes for all features. Eight of ten metrics show statistically significant differences, with five exhibiting large effect sizes ($|d| > 0.8$):

\begin{itemize}
    \item \textbf{Very large} ($d > 1.0$): Word count ($d = 2.07$), bullet points ($d = 1.44$), type-token ratio ($d = -1.06$), average word length ($d = 1.06$)
    \item \textbf{Large} ($0.8 \leq d < 1.0$): Flesch reading ease ($d = -0.82$)
    \item \textbf{Medium} ($0.5 \leq d < 0.8$): Sentence count ($d = 0.75$), formal word ratio ($d = 0.50$), Flesch-Kincaid grade ($d = 0.48$)
    \item \textbf{Not significant}: Average sentence length ($d = 0.25$), logical connectors ($d = -0.22$)
\end{itemize}

\begin{figure}[t]
\centering
\includegraphics[width=0.85\textwidth]{figures/effect_sizes.png}
\caption{Cohen's $d$ effect sizes for all extracted features. Positive values indicate higher values for \llmstyle{} prompts. Dashed lines indicate conventional thresholds for small ($|d| = 0.2$), medium ($|d| = 0.5$), and large ($|d| = 0.8$) effects.}
\label{fig:effect_sizes}
\end{figure}

\subsection{Model-Specific Analysis}

Table~\ref{tab:model_comparison} compares results between GPT-4.1-mini and Claude Sonnet 4. Both models show the same directional effects, but with different magnitudes.

\begin{table}[t]
\centering
\caption{Model-specific comparison of key metrics. Both models show consistent style mirroring effects, though GPT-4.1-mini exhibits larger length increases while Claude Sonnet 4 shows larger formality effects.}
\label{tab:model_comparison}
\small
\begin{tabular}{lcccccc}
\hline
& \multicolumn{3}{c}{\textbf{GPT-4.1-mini}} & \multicolumn{3}{c}{\textbf{Claude Sonnet 4}} \\
\hline
\textbf{Metric} & \human{} & \llmstyle{} & $p$-value & \human{} & \llmstyle{} & $p$-value \\
\hline
Word Count & 198.0 & 349.6 & 3.4e-20 & 191.2 & 296.6 & 2.6e-23 \\
Bullet Points & 6.7 & 17.1 & 6.3e-15 & 10.2 & 20.2 & 1.9e-12 \\
Formal Word Ratio & 0.08\% & 0.15\% & 0.030 & 0.09\% & 0.30\% & 1.8e-05 \\
\hline
\end{tabular}
\end{table}

\paragraph{GPT-4.1-mini} shows a larger length increase (+151.7 words, 77\% increase) compared to Claude Sonnet 4 (+105.4 words, 55\% increase). Both models dramatically increase bullet point usage in response to formal prompts.

\paragraph{Claude Sonnet 4} shows a larger formality increase (+0.21 percentage points in formal word ratio) compared to GPT-4.1-mini (+0.07 percentage points). This suggests Claude may be more sensitive to formal tone cues.

Figure~\ref{fig:model_comparison} visualizes the cross-model consistency of effects.

\begin{figure}[t]
\centering
\includegraphics[width=0.85\textwidth]{figures/model_comparison.png}
\caption{Comparison of style mirroring effects across GPT-4.1-mini and Claude Sonnet 4. Both models show consistent directional effects, with some variation in magnitude.}
\label{fig:model_comparison}
\end{figure}

\subsection{Qualitative Examples}

To illustrate the nature of style mirroring, we present example response excerpts for the same question.

\paragraph{Question: ``Why is the sky blue?''}

\textbf{\human{} prompt response} (truncated):
\begin{quote}
\small
\textit{Great question! The sky appears blue because of a phenomenon called \textbf{Rayleigh scattering}. Here's how it works:}
\begin{itemize}
\small
\item \textit{Sunlight is made up of many colors...}
\end{itemize}
\end{quote}

\textbf{\llmstyle{} prompt response} (truncated):
\begin{quote}
\small
\textit{Certainly! Here's a detailed and well-structured explanation of why the sky is blue:}

\textit{---}

\textit{\textbf{Why is the Sky Blue?}}

\textit{\underline{Introduction}}

\textit{The blue color of the sky is a common observation that has intrigued humans for centuries...}
\end{quote}

The \llmstyle{} response adds explicit section headers (``Introduction''), horizontal separators, and more elaborate structural organization---mirroring the formality requested in the prompt.

\subsection{Statistical Hypothesis Testing}

We formally test our hypotheses:

\textbf{$H_0$ (Null):} LLM responses are invariant to prompt style when semantic content is controlled.

\textbf{$H_1$ (Alternative):} LLMs exhibit measurable behavioral differences based on prompt style.

\textbf{Result: We strongly reject $H_0$.} Eight of ten metrics show statistically significant differences after Bonferroni correction ($p < 0.005$). Five metrics show large effect sizes ($|d| > 0.8$). Results are consistent across both model families tested.

\begin{figure}[t]
\centering
\includegraphics[width=0.85\textwidth]{figures/formality_analysis.png}
\caption{Analysis of formality-related features. \llmstyle{} prompts consistently elicit more formal responses with longer words, more formal vocabulary, and increased structural formatting.}
\label{fig:formality}
\end{figure}
