\relax 
\citation{brown2020gpt3,openai2023gpt4,anthropic2024claude}
\citation{guo2024multiagent}
\citation{guo2023hc3,dugan2024raid}
\citation{sharma2024sycophancy}
\citation{anagnostidis2024susceptible}
\@writefile{toc}{\contentsline {section}{\numberline {1}Introduction}{1}\protected@file@percent }
\citation{guo2023hc3}
\citation{dugan2024raid}
\citation{su2023hc3plus}
\citation{sharma2024sycophancy}
\@writefile{toc}{\contentsline {paragraph}{Research Question.}{2}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Our Contribution.}{2}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Paper Organization.}{2}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {2}Related Work}{2}\protected@file@percent }
\newlabel{sec:related}{{2}{2}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.1}Human vs. LLM Text Characteristics}{2}\protected@file@percent }
\citation{anagnostidis2024susceptible}
\citation{zheng2023personas}
\citation{wang2024persona}
\citation{guo2024multiagent}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.2}Sycophancy and User Adaptation}{3}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {2.3}Prompt Sensitivity and Influence}{3}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {2.4}Persona and Style Effects}{3}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {2.5}Multi-Agent LLM Systems}{3}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {2.6}Gap in Existing Work}{3}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {3}Methodology}{3}\protected@file@percent }
\newlabel{sec:methodology}{{3}{3}}
\citation{guo2023hc3,dugan2024raid}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.1}Prompt Pair Construction}{4}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Base Questions.}{4}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Stylistic Manipulation.}{4}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Example Prompt Pairs.}{4}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {3.2}Models and Experimental Setup}{4}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Models Tested.}{4}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{API Parameters.}{4}\protected@file@percent }
\citation{flesch1948readability}
\citation{kincaid1975derivation}
\citation{cohen1988statistical}
\@writefile{lot}{\contentsline {table}{\numberline {1}{\ignorespaces Example prompt pairs showing human-style and LLM-style versions of the same questions. Semantic content is preserved while stylistic features are systematically varied.}}{5}\protected@file@percent }
\newlabel{tab:prompt_examples}{{1}{5}}
\@writefile{toc}{\contentsline {paragraph}{Experimental Protocol.}{5}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {3.3}Feature Extraction}{5}\protected@file@percent }
\@writefile{lot}{\contentsline {table}{\numberline {2}{\ignorespaces Linguistic features extracted from LLM responses.}}{5}\protected@file@percent }
\newlabel{tab:features}{{2}{5}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.4}Statistical Analysis}{5}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Hypothesis Testing.}{5}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Effect Size.}{5}\protected@file@percent }
\citation{guo2023hc3}
\@writefile{lot}{\contentsline {table}{\numberline {3}{\ignorespaces Summary statistics comparing responses to \textsc  {Human-Style}{} versus \textsc  {LLM-Style}{} prompts. Results are aggregated across both GPT-4.1-mini and Claude Sonnet 4. Effect sizes (Cohen's $d$) and $p$-values are from paired $t$-tests with Bonferroni correction ($\alpha _{\text  {adj}} = 0.005$).}}{6}\protected@file@percent }
\newlabel{tab:main_results}{{3}{6}}
\@writefile{toc}{\contentsline {paragraph}{Reproducibility.}{6}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {4}Results}{6}\protected@file@percent }
\newlabel{sec:results}{{4}{6}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.1}Main Results}{6}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Key Finding 1: Response Length.}{6}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Key Finding 2: Structural Organization.}{6}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Key Finding 3: Reading Difficulty.}{6}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Key Finding 4: Vocabulary Patterns.}{6}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {1}{\ignorespaces Distribution of response word counts for \textsc  {Human-Style}{} versus \textsc  {LLM-Style}{} prompts. Responses to LLM-style prompts are consistently longer across both models, with minimal overlap between distributions.}}{7}\protected@file@percent }
\newlabel{fig:length_comparison}{{1}{7}}
\@writefile{lof}{\contentsline {figure}{\numberline {2}{\ignorespaces Cohen's $d$ effect sizes for all extracted features. Positive values indicate higher values for \textsc  {LLM-Style}{} prompts. Dashed lines indicate conventional thresholds for small ($|d| = 0.2$), medium ($|d| = 0.5$), and large ($|d| = 0.8$) effects.}}{7}\protected@file@percent }
\newlabel{fig:effect_sizes}{{2}{7}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.2}Effect Size Analysis}{7}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {4.3}Model-Specific Analysis}{7}\protected@file@percent }
\@writefile{lot}{\contentsline {table}{\numberline {4}{\ignorespaces Model-specific comparison of key metrics. Both models show consistent style mirroring effects, though GPT-4.1-mini exhibits larger length increases while Claude Sonnet 4 shows larger formality effects.}}{8}\protected@file@percent }
\newlabel{tab:model_comparison}{{4}{8}}
\@writefile{lof}{\contentsline {figure}{\numberline {3}{\ignorespaces Comparison of style mirroring effects across GPT-4.1-mini and Claude Sonnet 4. Both models show consistent directional effects, with some variation in magnitude.}}{8}\protected@file@percent }
\newlabel{fig:model_comparison}{{3}{8}}
\@writefile{toc}{\contentsline {paragraph}{GPT-4.1-mini}{8}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Claude Sonnet 4}{8}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {4.4}Qualitative Examples}{8}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Question: ``Why is the sky blue?''}{8}\protected@file@percent }
\citation{guo2023hc3}
\citation{giles1991accommodation}
\@writefile{lof}{\contentsline {figure}{\numberline {4}{\ignorespaces Analysis of formality-related features. \textsc  {LLM-Style}{} prompts consistently elicit more formal responses with longer words, more formal vocabulary, and increased structural formatting.}}{9}\protected@file@percent }
\newlabel{fig:formality}{{4}{9}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.5}Statistical Hypothesis Testing}{9}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {5}Discussion}{9}\protected@file@percent }
\newlabel{sec:discussion}{{5}{9}}
\@writefile{toc}{\contentsline {subsection}{\numberline {5.1}Interpretation: The Style Mirroring Effect}{9}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {5.2}Possible Mechanisms}{9}\protected@file@percent }
\citation{sharma2024sycophancy}
\citation{guo2024multiagent}
\@writefile{toc}{\contentsline {paragraph}{Instruction Following.}{10}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Training Distribution Matching.}{10}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Sycophantic Adaptation.}{10}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Implicit Source Detection.}{10}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {5.3}Implications}{10}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{For Prompt Engineering.}{10}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{For Multi-Agent Systems.}{10}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{For AI Safety.}{10}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {5.4}Surprising Findings}{10}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Logical Connectors Decreased.}{10}\protected@file@percent }
\citation{sharma2024sycophancy}
\citation{anagnostidis2024susceptible}
\citation{zheng2023personas}
\@writefile{toc}{\contentsline {paragraph}{Vocabulary Diversity Decreased.}{11}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Model-Specific Patterns.}{11}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {5.5}Limitations}{11}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Prompt Template Variety.}{11}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Model Coverage.}{11}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Single Run Design.}{11}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Style Manipulation Validity.}{11}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Causal Mechanism Unclear.}{11}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Ecological Validity.}{11}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {5.6}Relation to Prior Work}{11}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {6}Conclusion}{11}\protected@file@percent }
\newlabel{sec:conclusion}{{6}{11}}
\bibstyle{plainnat}
\bibdata{references}
\bibcite{anagnostidis2024susceptible}{{1}{2024}{{Anagnostidis and Bulian}}{{}}}
\bibcite{anthropic2024claude}{{2}{2024}{{Anthropic}}{{}}}
\bibcite{brown2020gpt3}{{3}{2020}{{Brown et~al.}}{{Brown, Mann, Ryder, Subbiah, Kaplan, Dhariwal, Neelakantan, Shyam, Sastry, Askell, et~al.}}}
\bibcite{cohen1988statistical}{{4}{1988}{{Cohen}}{{}}}
\bibcite{dugan2024raid}{{5}{2024}{{Dugan et~al.}}{{Dugan, Hwang, Trhl{\'\i }k, Zhu, Luber, Ippolito, and Callison-Burch}}}
\bibcite{flesch1948readability}{{6}{1948}{{Flesch}}{{}}}
\bibcite{giles1991accommodation}{{7}{1991}{{Giles et~al.}}{{Giles, Coupland, and Coupland}}}
\bibcite{guo2023hc3}{{8}{2023}{{Guo et~al.}}{{Guo, Zhang, Wang, Jiang, Nie, Ding, Yue, and Wu}}}
\@writefile{toc}{\contentsline {paragraph}{Key Takeaways.}{12}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Future Work.}{12}\protected@file@percent }
\bibcite{guo2024multiagent}{{9}{2024}{{Guo et~al.}}{{Guo, Chen, Wang, Chang, Pei, Chawla, Wiest, and Zhang}}}
\bibcite{kincaid1975derivation}{{10}{1975}{{Kincaid et~al.}}{{Kincaid, Fishburne~Jr, Rogers, and Chissom}}}
\bibcite{openai2023gpt4}{{11}{2023}{{OpenAI}}{{}}}
\bibcite{sharma2024sycophancy}{{12}{2024}{{Sharma et~al.}}{{Sharma, Tong, Korbak, Duvenaud, Askell, Bowman, Cheng, Durmus, Hatfield-Dodds, Johnston, et~al.}}}
\bibcite{su2023hc3plus}{{13}{2023}{{Su et~al.}}{{Su, Wang, and Chen}}}
\bibcite{wang2024persona}{{14}{2024}{{Wang et~al.}}{{Wang, Lyu, Wei, et~al.}}}
\bibcite{zheng2023personas}{{15}{2023}{{Zheng et~al.}}{{Zheng, Pei, and Jurgens}}}
